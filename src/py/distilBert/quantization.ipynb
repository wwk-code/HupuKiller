{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/workspace/anaconda/envs/HupuKiller/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os,sys,torch,time\n",
    "\n",
    "# 将本地包路径添加到 sys.path 的最前面\n",
    "local_packages_path = os.path.abspath(\"/data/workspace/projects/HupuKiller/local_libs/site-packages\")\n",
    "sys.path.insert(0, local_packages_path)\n",
    "\n",
    "import torch\n",
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import  QConfig,QConfigMapping\n",
    "from transformers import pipeline,AutoTokenizer\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.ao.quantization.observer import HistogramObserver,default_observer,default_weight_observer\n",
    "\n",
    "\n",
    "# 项目根目录\n",
    "project_root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "# 项目python源码目录\n",
    "py_src_root_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(py_src_root_dir)\n",
    "\n",
    "from common.myFile import * \n",
    "from customDistilBert import CustomDistilBert \n",
    "\n",
    "from distilBertInfer import getDataLoaderAndLabels\n",
    "\n",
    "\n",
    "# 自定义 HistogramObserver (目前的计算图IR中存在将torch.Size作为HistogramObserver输入的情况，需要特殊处理)\n",
    "class CustomHistogramObserver(HistogramObserver):\n",
    "    def forward(self, x_orig):\n",
    "        # 如果输入是 torch.Size，直接返回,不进行直方图信息统计\n",
    "        if isinstance(x_orig, torch.Size):\n",
    "            return x_orig\n",
    "        else: \n",
    "            return super().forward(x_orig)\n",
    "\n",
    "\n",
    "# custom_qconfig = QConfig(\n",
    "#     activation=CustomHistogramObserver.with_args(dtype=torch.qint8,reduce_range=True),\n",
    "#     weight = default_weight_observer\n",
    "# )\n",
    "\n",
    "custom_qconfig = get_default_qconfig(\"x86\")\n",
    "\n",
    "\n",
    "def do_fx_mode_ptq():\n",
    "\n",
    "    with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
    "        device = torch.device('cpu')\n",
    "        model_path = '/data/workspace/projects/HupuKiller/outputs/DistilBert/fineTune'\n",
    "        bert_base = BertForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        customDistilBert = CustomDistilBert(bert_base).to(device)\n",
    "        checkpointFilePath = '/data/workspace/projects/HupuKiller/outputs/DistilBert/distillation/model_epoch_4.pth'\n",
    "        distilCheckpoints = torch.load(checkpointFilePath,map_location=device)\n",
    "        customDistilBert.load_state_dict(distilCheckpoints)\n",
    "\n",
    "        model = customDistilBert\n",
    "        model.eval()\n",
    "\n",
    "        dataLoader,rawLabels = getDataLoaderAndLabels(batchSize = 8,tokenizer=tokenizer,device=device)\n",
    "        # qconfig = get_default_qconfig(\"x86\")\n",
    "        qconfig_mapping = QConfigMapping().set_global(custom_qconfig)\n",
    "\n",
    "        # 打印 CUDA 操作日志\n",
    "        # print(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n",
    "\n",
    "        def calibrate(model: torch.nn.Module, data_loader: DataLoader):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in dataLoader:\n",
    "                    batch_input_ids, batch_attention_mask = batch\n",
    "\n",
    "                    # 新添加逻辑,为了可以通过 torch.ao.quantization.HistogramObserver 的观测\n",
    "                    batch_attention_mask = batch_attention_mask.float()  # 为了解决某个报错\n",
    "\n",
    "                    model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "\n",
    "        example_inputs = (next(iter(dataLoader))[0]) # get an example input\n",
    "        prepared_model = prepare_fx(model, qconfig_mapping,example_inputs)  # fuse modules and insert observers\n",
    "        calibrate(prepared_model, dataLoader)  # run calibration on sample data\n",
    "        quantized_model = convert_fx(prepared_model)  # convert the calibrated model to a quantized model\n",
    "\n",
    "        return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_197749/93579053.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  distilCheckpoints = torch.load(checkpointFilePath,map_location=device)\n",
      "/data/workspace/anaconda/envs/HupuKiller/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ptq_model = do_fx_mode_ptq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.module.Module"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ptq_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_1 = next(ptq_model.encoder.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.module.Module'>\n",
      "Module(\n",
      "  (attention): Module(\n",
      "    (self): Module(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): Module(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): Module(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): Module(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(type(encoder_1))\n",
    "print(encoder_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(type(encoder_1.attention.self.value))\n",
    "print(encoder_1.attention.self.value.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:0 ; child: Module(\n",
      "  (attention): Module(\n",
      "    (self): Module(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): Module(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): Module(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): Module(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "name:1 ; child: Module(\n",
      "  (attention): Module(\n",
      "    (self): Module(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): Module(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): Module(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): Module(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "name:2 ; child: Module(\n",
      "  (attention): Module(\n",
      "    (self): Module(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): Module(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): Module(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): Module(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "name:3 ; child: Module(\n",
      "  (attention): Module(\n",
      "    (self): Module(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): Module(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): Module(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): Module(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "name:4 ; child: Module(\n",
      "  (attention): Module(\n",
      "    (self): Module(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): Module(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): Module(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): Module(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "name:5 ; child: Module(\n",
      "  (attention): Module(\n",
      "    (self): Module(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): Module(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): Module(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): Module(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for name, child in ptq_model.encoder.named_children():\n",
    "    print(f\"name:{name} ; child: {child}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GraphModule' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m ptq_model\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Quantized Weight: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Quantized Bias: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39mbias()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/workspace/anaconda/envs/HupuKiller/lib/python3.10/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GraphModule' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "for name, module in ptq_model.named_modules():\n",
    "    print(f\"Module: {name}\")\n",
    "    print(f\"  Quantized Weight: {module.weight()}\")\n",
    "    print(f\"  Quantized Bias: {module.bias()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HupuKiller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
