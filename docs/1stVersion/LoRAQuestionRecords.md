#### **1.1. Prompt-Based Finetuning 的关键**

1. **构造明确的 Prompt** ：在输入中嵌入任务相关的上下文信息，使模型更好地理解目标。

* **Example** :
  * Prompt: "请提供以下球员的总决赛场均数据。输入: {球员: 贾马尔-穆雷, 年份: 2023}"
  * 输出: "贾马尔-穆雷 在 2023 年 NBA 总决赛的场均数据如下: ..."

1. **统一格式和逻辑** ：使用标准化的输入和输出格式，减少模型学习的复杂度。
2. **减少参数改动** ：通过 LoRA 等方法仅优化少量权重（如偏置、部分层），避免对全模型进行大规模更新。

#### **1.3. 针对此场景的建议**

* **任务特点** ：你的任务主要关注于小规模数值型事实微调，数据稀疏且高精度要求。
* **建议参数设置** ：
* **lora_rank** ：
  * 建议值：`rank=16`
  * 原因：低秩表示的复杂度适中，能有效捕获具体数值细节。
* **lora_alpha** ：
  * 建议值：`alpha=16~32`
  * 原因：保证权重更新平稳，避免因数据量少导致模型偏差。

> 若实验结果不理想，可在 `rank=8~32` 和 `alpha=16~64` 间进行网格搜索优化。



#### **2.2. 如何在用户正常聊天形式下自动引导模型**

为了让用户能够以自然语言提问，同时利用 Prompt 提升模型准确率，可以引入 **系统层的提示模板** 或  **后台处理逻辑** (针对部分高精度需求的数据做mapping硬编码)。
